---
title: Math and Number Theory for CP
---

<h1 class="font-bold text-white">Some important facts</h1>

- Number of *distinct* prime factors of $n$ is at most $O(\log{n})$
    * Reasoning: $n < 10^9$ with the most distinct prime factors is 
        $2\cdot 3\cdot 5\cdot 7\cdot 11\cdot 13\cdot 17\cdot 19\cdot 23 = 223092870$ with 9 distinct prime factors
- Number of factors of $n$ is approximately at most $O(\sqrt[3]n)$
    * Reasoning: the maximum amount of divisors of a number less than 1E6 is 240
        (keep in mind that numbers with a large amount of divisors arenâ€™t common)
- $n$th harmonic number is approximately $\ln(n)$ aka $\frac{1}{1} + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + ... + \frac{1}{n} \approx \ln(n)$
    * Reasoning: (Calculus) $\sum_{i=1}^{n}\limits{\frac{1}{i}} \leq \int_1^n\limits \frac{1}{x} dx+1 = \ln{n}+1$

<h1 class="font-bold text-white">Prime Factorization</h1>

Given an integer $n$, how can we efficiently find its prime factors and their frequencies?

Notice that the smallest divisor of $n$ must always be prime. If it isn't prime, then there exists a prime factor of that divisor but that's a contradiction since its supposed to be the smallest divisor.
Therefore, we just need to find the smallest divisor of $n$, remove all occurences of it, then repeat.

Also consider a factorization of $n = p\cdot q$ where $p \leq q$. <br />
$p$ must lie within the interval $[1,\sqrt n]$ and $q$ must lie within the interval $[\sqrt n, n]$ because otherwise, we'd have contradictions like $n < n$ or $n > n$. <br />
If there doesn't exist any prime factor in the range $[1,\sqrt n]$, then $n$ must be prime since the only factorization is the trivial case <br /> $1\cdot n = n$.

Using these observations, we can create a recursive algorithm that finds the smallest divisor of $n$, removes its frequency (let's call it $k$) from $n$, and repeats the algorithm on $\frac{n}{d^k}$. However, there's a much more elegant and efficient implementation:
<cpp>
vector&ltint&gt factorize(int n) {
    vector&ltint&gt factors;
    for (int d = 2; d * d &lt= n; d++) {
        while (n % d == 0) {
            factors.push_back(d);
            n /= d;
        }
    }
    if (n &gt 1) factors.push_back(n);
    return factors;
}
</cpp>
Using this iterative implementation, we can clearly give this factorization algorithm an upperbound of $O(\sqrt n)$.<br>

<h1 class="font-bold text-white">Sieve of Eratosthenes</h1>

How can we efficiently find all primes less than $N$?

As the name suggests, we can "sieve" through all the numbers from 2 to $N$. <br>
Let's start by assuming all integers from $2$ to $N$ are prime. <br>
Take the first value marked as a prime which would be $2$ and mark all multiples of $2$ as composite.
Then, find the next marked prime which would be $3$ and mark $6, 9, 12, ...$ as composite.
Note that we skip $4$ since it is marked as composite instead of prime.

For time complexity, note that we perform $N + \frac{N}{2} + \frac{N}{3} + \frac{N}{5} + \frac{N}{7} + ...$ operations.<br>
Note that the sum of all reciprocals of integers is approximated as $\ln(N)$ but here, we have the sum of all reciprocals of primes.
Using the [Prime Number Theorem](https://en.wikipedia.org/wiki/Prime_number_theorem), we can show that this ends up simplifying to $O(N\log\log N)$.

This algorithm can be extended to solve other problems, such as finding and storing all divisors of integers from $1$ to $N$, since the core idea is essentially the same (although the time complexity becomes $O(N\log N)$ since we no longer skip composite numbers). Variants of the Sieve of Eratosthenes are also sometimes called "divisor loop."

There are some optimizations we can employ such as marking multiples of $p$ starting from $p^2$ since any multiple of $p$ less than $p^2$ would already be marked as composite from previous primes. However, these optimizations won't always apply to every variant of Sieve and the overall time complexity remains the same.

Implementation:
<cpp>
vector&ltbool&gt prime(N, true);//assume all values are prime
vector&ltint&gt primes;
for (int i = 2; i &lt N; i++) {
    if (prime[i] && (long long)i*i &lt N) {
        primes.push_back(i);
        for (int j = i*i; j &lt N; j += i) {
            prime[j] = false;
        }
    }
}
</cpp>

<!---
<h1 class="font-bold text-white">Binary Exponentation</h1>

(Also known as modular exponentiation, fast exponentiation, exponentiation by squaring depending on the context)

How can we calculate $b^k \pmod m$* efficiently?<br>
*Note: this assumes reader is adequately familiar with modular arithmetic

Observe that when we square numbers, $(b^a)^2 = b^{2a} \pmod m$, we multiply its power by two.
Using this, we can easily calculate values in the form $b^{2^a} \pmod m$ by repeatedly squaring $a$ times.

How can we use this for exponents that aren't powers of two?<br>
Notice that any arbitary number can be expressed as a sum of powers of two by converting to binary.<br>
For example, $11 = 8 + 2 + 1 = 2^3 + 2^1 + 2^0$.
Therefore, $b^{11} = b^{2^3 + 2^1 + 2^0} = b^{2^3} \cdot b^{2^1} \cdot b^{2^0} \pmod m$.

Hence, we can keep a separate variable that calculates $b^{2^a} \pmod m$ and multiply that to our answer whenever $k$ contains that power of two.

<cpp>
using ll = long long;
ll bExpo(ll b, ll k, ll m) {
    b %= m, k %= m;
    ll ans = 1;
    while (k != 0) {
        if (k%2 == 1) ans = (ans*b)%m;
        b = (b * b)%m;
        k /= 2;
    }
    return ans;
}
</cpp>

<h1 class="font-bold text-white">Euclidean GCD</h1>

How can we calculate $\gcd(a,b)$ efficiently?<br>
(WLOG, assume $a < b$)

We *could* prime factorize $a,b$ and take the minimum of the exponents and multiply them back together but that's <br>$O(\sqrt a + \sqrt b)$. We can do better!

The important observation is that $\gcd(a,b) = \gcd(a,b-a)$.
The proof for this is left to the reader.

We can conclude $\gcd(a,b) = \gcd(a, b \pmod a)$ since taking a modulo is the same as repeated subtraction. Then, we just keep repeating this to reduce the value of the two inputs.
The point where we can no longer reduce $a,b$ is when one of them becomes $0$, in which case we can return the nonzero value of the other.

For example, $\gcd(15,20) = \gcd(15, 5) = \gcd(5, 15) = \gcd(5, 0) = \gcd(0, 5) = 5$.

In order to analyze the time complexity, consider the worst case where we need to maximize the number of iterations to reduce $a$ to $0$. This happens when `b%a = b-a`. Using induction with $a = 1, b = 1$, we can show that the number of iterations is maximized when $a,b$ are consecutive Fibonacci numbers. Since the Fibonacci sequence grows exponentially, the number of iterations grows logarithmicly. <br>
Therefore, the worst case time complexity of Euclidean GCD is $O(\log n)$ although in practice, it is quite fast.

Using the fact that $a\cdot b = \text{lcm}(a,b) \cdot \gcd(a,b)$, we can also efficiently calculate LCM's.

In C++ or Python, there exists a built-in GCD function but for Java, you have to implement it yourself. This algorithm can also be [extended](https://en.wikipedia.org/wiki/Extended_Euclidean_algorithm) to solve linear systems.

<cpp>
int gcd (int a, int b) {
    if (b == 0) return a;
    else return gcd (b, a % b);
}
</cpp>--->